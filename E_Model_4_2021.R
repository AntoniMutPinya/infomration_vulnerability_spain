
### DATA & PACKAGES

source('A_Data_&_Packages.R')

################################################################################
### INDEPENDENT VARIABLE (X)
################################################################################

# Calculate the Information Vulnerability Index (IV) using data related to financial literacy, basic numeracy,
# and graph reading skills. This index is estimated using an Item Response Theory (IRT) model.

# Min-Max scaling function for normalization
min_max_scale <- function(x) {
  min_range <- 0
  max_range <- 1
  min_x <- min(x)
  max_x <- max(x)
  min_range + ((x - min_x) / (max_x - min_x)) * (max_range - min_range)
}

# Function to create the answers dataframe used for IRT-based index calculation
create_df_Answers <- function(ecf_2021) {
  
  # Dataframe creation using conditional logic for Financial Literacy (FL), Numeracy (NL), and Graph Reading
  df_Answers <- data.frame(
    
    
    # Financial Literacy questions
    FL1 = ifelse(ecf_2021$e0600 == 3, 1, 0),
    FL2 = ifelse(ecf_2021$e0700 == 0, 1, 0),
    FL3 = ifelse(ecf_2021$e0800 == 102, 1, 0),
    FL4 = ifelse(ecf_2021$e0900 == 1, 1, 0),
    FL5 = ifelse(ecf_2021$e1001 == 1, 1, 0),
    FL6 = ifelse(ecf_2021$e1002 == 1, 1, 0),
    FL7 = ifelse(ecf_2021$e1003 == 1, 1, 0),
    FL8 = ifelse(ecf_2021$e1200 == 1, 1, 0),
    
    # Basic Numeracy and Literacy
    NL1 = ifelse(ecf_2021$e0500 == 200, 1, 0),
    NL2 = ifelse(ecf_2021$e1101 == 1, 1, 0),
    NL3 = ifelse(ecf_2021$e1400 == 10, 1, 0),
    NL4 = ifelse(ecf_2021$e0401 == 3, 1, 0),
    NL5 = ifelse(ecf_2021$e0402 == 18, 1, 0),
    NL6 = ifelse(ecf_2021$e1500 == 30, 1, 0),
    NL7 = ifelse(ecf_2021$e0300 == 5, 1, 0),
    NL8 = ifelse(ecf_2021$e1600 == 4, 1, 0),
    
    # Filter variable
    filter = ecf_2021$b0600
  )
  
  # Filter out invalid data (-98 values)
  df_Answers <- df_Answers[df_Answers$filter != -98, ]
  return(df_Answers)
}

# Function to fit a 2PL model and calculate the Information Vulnerability Index
create_index <- function(df_Answers, unimodel = 'F1 = 1-16') {
  
  # Select financial literacy and numeracy columns
  columns_FL <- grep("^FL|NL", names(df_Answers), value = TRUE)
  df_FL <- df_Answers[, columns_FL]
  
  # Fit the 2PL IRT model
  fit2PL <- mirt(data = df_FL, model = unimodel, itemtype = "2PL", verbose = FALSE)
  
  # Extract the factor scores (theta) as the index
  IV <- fscores(fit2PL)
  
  return(IV)
}

# Generate the IV index
df_Answers <- create_df_Answers(ecf_2021)
IV <- min_max_scale(create_index(df_Answers))
IV <- as.data.frame(IV)
IV <- IV$F1

# Clean up variables
rm(df_Answers, create_df_Answers, create_index)

################################################################################
### DEPENDENT VARIABLE (Y)
################################################################################

# Select specific columns (b0720a to b0720i) from ecf_2021 and store in df
df <- ecf_2021 %>% dplyr::select(b0720a,
                                 b0720b,
                                 b0720c,
                                 b0720d,
                                 b0720e,
                                 b0720f,
                                 b0720g,
                                 b0720h,
                                 b0720i)

# NOT OK
# Apply binary transformation for columns b0720b to b0720i
# If the value is negative, set it to NA (missing); if greater than 0, set it to -1, otherwise 0
df$b0720c <- ifelse(df$b0720c == -98, NA, ifelse(df$b0720c > 0, -1, 0))
df$b0720d <- ifelse(df$b0720d == -98, NA, ifelse(df$b0720d > 0, -1, 0))
df$b0720e <- ifelse(df$b0720e == -98, NA, ifelse(df$b0720e > 0, -1, 0))
df$b0720f <- ifelse(df$b0720f == -98, NA, ifelse(df$b0720f > 0, -1, 0))
df$b0720g <- ifelse(df$b0720g == -98, NA, ifelse(df$b0720g > 0, -1, 0))
df$b0720h <- ifelse(df$b0720h == -98, NA, ifelse(df$b0720h > 0, -1, 0))
df$b0720i <- ifelse(df$b0720i == -98, NA, ifelse(df$b0720i > 0, -1, 0))

# OK
# Apply transformation for columns b0720a to b0720i
# If the value is negative, set it to NA; if greater than 0, set it to +1, otherwise 0
df$b0720a <- ifelse(df$b0720a == -98, NA, ifelse(df$b0720a > 0, 1, 0))
df$b0720b <- ifelse(df$b0720b == -98, NA, ifelse(df$b0720b > 0, 1, 0))

# Create two new columns summing binary variables
# Fuentes_OK: Sum of columns b0700b to b0700i (positive variables)
df$Fuentes_OK <- df$b0720a + df$b0720b

# Fuentes_No_OK: Sum of columns b0700a to b0700r (negative variables)
df$Fuentes_No_OK <- df$b0720c + df$b0720d + df$b0720e + df$b0720f + df$b0720g + df$b0720h + df$b0720i 

# Select the two calculated columns into a new dataframe df_DV_b
df_DV_b <- df %>% select(Fuentes_OK, Fuentes_No_OK)

# Calculate DV3: Fuentes_OK minus Fuentes_No_OK, set values greater than 1 to 1, others to 0
DV4 <- ifelse(df$Fuentes_OK >= 1, 1, 0)
DV4 <- na.omit(DV4)

rm(df_DV_b)


################################################################################
### CONTROL VARIABLES (PCA)
################################################################################

# Perform PCA on selected sociodemographic variables and generate 4 components (I1, I2, I3, I4) as control variables.

# Create the dataset for PCA analysis
BE <- data.frame(
  Filter = ecf_2021$b0600,
  Q1 = ifelse(ecf_2021$d0101 > 0, ecf_2021$d0101, 0),
  Q2 = ifelse(ecf_2021$d0102 > 0, ecf_2021$d0102, 0),
  Q3 = ifelse(ecf_2021$d0103 > 0, ecf_2021$d0103, 0),
  Q4 = ifelse(ecf_2021$d0104 > 0, ecf_2021$d0104, 0),
  Q5 = ifelse(ecf_2021$d0105 > 0, ecf_2021$d0105, 0),
  Q6 = ifelse(ecf_2021$d0106 > 0, ecf_2021$d0106, 0),
  Q7 = ifelse(ecf_2021$d0107 > 0, ecf_2021$d0107, 0),
  Q8 = ifelse(ecf_2021$d0108 > 0, ecf_2021$d0108, 0),
  Q9 = ifelse(ecf_2021$d0109 > 0, ecf_2021$d0109, 0),
  Q10 = ifelse(ecf_2021$d0110 > 0, ecf_2021$d0110, 0),
  Q11 = ifelse(ecf_2021$d0111 > 0, ecf_2021$d0111, 0),
  Q12 = ifelse(ecf_2021$d0112 > 0, ecf_2021$d0112, 0)
)

# Remove invalid data
BE <- BE[BE$Filter != -98, ]

# Perform PCA analysis and extract 4 components
mat_cor <- hetcor(BE)$correlations  # Correlation matrix
fa_FL <- fa(BE, nfactors = 4, rotate = 'none', fm = 'pa', max.iter = 1, scores = "Bartlett")
df_Inst <- as.data.frame(fa_FL$scores)

# Apply Min-Max scaling to the factor scores
I1 <- min_max_scale(df_Inst$PA1)
I2 <- min_max_scale(df_Inst$PA2)
I3 <- min_max_scale(df_Inst$PA3)
I4 <- min_max_scale(df_Inst$PA4)

# Clean memory
rm(fa_FL, mat_cor, df_Inst, BE)

################################################################################
### INSTRUMENTAL VARIABLES
################################################################################

# Create the instrumental variables to address potential endogeneity in regression models.

# IV_1: Binary indicator based on the number of books in childhood (a1400 >= 4)
InsV_1 <- ifelse(ecf_2021$a1400 >= 4, 1, 0)

# IV_2: Binary indicator for specific university qualifications requiring mathematical or financial knowledge
InsV_2 <- ifelse(ecf_2021$a1200 %in% c(1, 2, 4, 5, 6), 1, 0)

df <- data.frame(  
  InsV_1 = InsV_1,
  InsV_2 = InsV_2,
  Filter = ecf_2021$b0600
)

df <- df[df$Filter != -98, ]

InsV_1 <- df$InsV_1
InsV_2 <- df$InsV_2

# Clean memory
rm(df)

################################################################################
### REGRESSION (IV Models)
################################################################################

# Perform Instrumental Variable (IV) regression to estimate the effect of the endogenous independent variable on the dependent variable.

# Model 4.1: Using Instrumental Variable 1 (Cultural Level)
data <- data.frame(
  Y = DV4,
  X = IV,
  Z = InsV_1,
  C_1 = I1,
  C_2 = I2,
  C_3 = I3,
  C_4 = I4
)

fitY.LX <- glm(Y ~ X + C_1 + C_2 + C_3 + C_4, family = "binomial", data = data)
fitX.LZ <- glm(X ~ Z + C_1 + C_2 + C_3 + C_4, data = data)
fitIV_ts <- ivglm(estmethod = "ts", fitX.LZ = fitX.LZ, fitY.LX = fitY.LX, data = data)
summary(fitIV_ts)

#  Call:  
#  ivglm(estmethod = "ts", fitX.LZ = fitX.LZ, fitY.LX = fitY.LX, data = data)

#  Coefficients: 
#  Estimate Std. Error z value Pr(>|z|)    
#  (Intercept)  -5.7402     0.8225  -6.979 2.98e-12 ***
#  X             4.5344     1.2654   3.583 0.000339 ***
#  C_1           0.8559     0.4487   1.908 0.056450 .  
#  C_2           0.2661     0.4903   0.543 0.587369    
#  C_3           1.8301     0.3875   4.723 2.33e-06 ***
#  C_4          -1.3542     0.5286  -2.562 0.010418 *  
#  ---
#  Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

coeficients <- as.data.table(fitIV_ts$est)
coeficients <- transpose(coeficients)

# Define the coefficients from the model
intercept <- coeficients$V1
beta_X <- coeficients$V2
beta_C1 <- coeficients$V3
beta_C2 <- coeficients$V4
beta_C3 <- coeficients$V5
beta_C4 <- coeficients$V6

# Calculate the linear predictor (logit) for each row
data$linear_predictor <- intercept + 
  (beta_X * data$X) + 
  (beta_C1 * data$C_1) + 
  (beta_C2 * data$C_2) + 
  (beta_C3 * data$C_3) + 
  (beta_C4 * data$C_4)

# Convert the linear predictor to predicted probabilities using the logistic function
data$predicted_prob <- 1 / (1 + exp(-data$linear_predictor))

# Run the Hosmer-Lemeshow test
hoslem_test <- hoslem.test(data$Y, data$predicted_prob)  # g=10 divides data into deciles

# Print the test result
print(hoslem_test)

# Model 4.2: Using Instrumental Variable 2 (Mathematical Baggage)
data <- data.frame(
  Y = DV4,                 # Dependent variable
  X = IV,                  # Endogenous independent variable
  Z = InsV_2,              # Instrumental variable
  C_1 = I1,                # Control variable 1
  C_2 = I2,                # Control variable 2
  C_3 = I3,                # Control variable 3
  C_4 = I4                 # Control variable 4  
)

fitY.LX <- glm(formula = Y ~ X + C_1 + C_2 + C_3 + C_4, family = "binomial", data = data)
fitX.LZ <- glm(formula = X ~ Z + C_1 + C_2 + C_3 + C_4, data = data)
fitIV_ts <- ivglm(estmethod="ts", fitX.LZ=fitX.LZ, fitY.LX=fitY.LX, data = data)
summary(fitIV_ts)

#  Call:  
#  ivglm(estmethod = "ts", fitX.LZ = fitX.LZ, fitY.LX = fitY.LX, data = data)

#  Coefficients: 
#  Estimate Std. Error z value Pr(>|z|)    
#  (Intercept)  -5.3115     0.6870  -7.731 1.07e-14 ***
#  X             3.6976     0.8603   4.298 1.72e-05 ***
#  C_1           0.9979     0.4028   2.477  0.01324 *  
#  C_2           0.4203     0.4610   0.912  0.36196    
#  C_3           1.8192     0.3837   4.741 2.13e-06 ***
#  C_4          -1.4418     0.5258  -2.742  0.00611 ** 
#  ---
#  Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

coeficients <- as.data.table(fitIV_ts$est)
coeficients <- transpose(coeficients)

# Define the coefficients from the model
intercept <- coeficients$V1
beta_X <- coeficients$V2
beta_C1 <- coeficients$V3
beta_C2 <- coeficients$V4
beta_C3 <- coeficients$V5
beta_C4 <- coeficients$V6

# Calculate the linear predictor (logit) for each row
data$linear_predictor <- intercept + 
  (beta_X * data$X) + 
  (beta_C1 * data$C_1) + 
  (beta_C2 * data$C_2) + 
  (beta_C3 * data$C_3) + 
  (beta_C4 * data$C_4)

# Convert the linear predictor to predicted probabilities using the logistic function
data$predicted_prob <- 1 / (1 + exp(-data$linear_predictor))

# Run the Hosmer-Lemeshow test
hoslem_test <- hoslem.test(data$Y, data$predicted_prob)  # g=10 divides data into deciles

# Print the test result
print(hoslem_test)

summary(DV4)

