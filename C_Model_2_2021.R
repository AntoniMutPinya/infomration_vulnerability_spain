
### DATA & PACKAGES

source('A_Data_&_Packages.R')
ecf_2016 <- ecf_2021

################################################################################
### INDEPENDENT VARIABLE (X)
################################################################################

# Calculate the Information Vulnerability Index (IV) using data related to financial literacy, basic numeracy,
# and graph reading skills. This index is estimated using an Item Response Theory (IRT) model.

# Min-Max scaling function for normalization
min_max_scale <- function(x) {
  min_range <- 0
  max_range <- 1
  min_x <- min(x)
  max_x <- max(x)
  min_range + ((x - min_x) / (max_x - min_x)) * (max_range - min_range)
}

# Function to create the answers dataframe used for IRT-based index calculation
create_df_Answers <- function(ecf_2016) {
  
  # Dataframe creation using conditional logic for Financial Literacy (FL), Numeracy (NL), and Graph Reading
  df_Answers <- data.frame(
    
    
    # Financial Literacy questions
    FL1 = ifelse(ecf_2016$e0600 == 3, 1, 0),
    FL2 = ifelse(ecf_2016$e0700 == 0, 1, 0),
    FL3 = ifelse(ecf_2016$e0800 == 102, 1, 0),
    FL4 = ifelse(ecf_2016$e0900 == 1, 1, 0),
    FL5 = ifelse(ecf_2016$e1001 == 1, 1, 0),
    FL6 = ifelse(ecf_2016$e1002 == 1, 1, 0),
    FL7 = ifelse(ecf_2016$e1003 == 1, 1, 0),
    FL8 = ifelse(ecf_2016$e1200 == 1, 1, 0),
    
    # Basic Numeracy and Literacy
    NL1 = ifelse(ecf_2016$e0500 == 200, 1, 0),
    NL2 = ifelse(ecf_2016$e1101 == 1, 1, 0),
    NL3 = ifelse(ecf_2016$e1400 == 10, 1, 0),
    NL4 = ifelse(ecf_2016$e0401 == 3, 1, 0),
    NL5 = ifelse(ecf_2016$e0402 == 18, 1, 0),
    NL6 = ifelse(ecf_2016$e1500 == 30, 1, 0),
    NL7 = ifelse(ecf_2016$e0300 == 5, 1, 0),
    NL8 = ifelse(ecf_2016$e1600 == 4, 1, 0),
    
    # Filter variable
    filter = ecf_2016$b0600
  )
  
  # Filter out invalid data (-98 values)
  df_Answers <- df_Answers[df_Answers$filter != -98, ]
  return(df_Answers)
}

# Function to fit a 2PL model and calculate the Information Vulnerability Index
create_index <- function(df_Answers, unimodel = 'F1 = 1-16') {
  
  # Select financial literacy and numeracy columns
  columns_FL <- grep("^FL|NL", names(df_Answers), value = TRUE)
  df_FL <- df_Answers[, columns_FL]
  
  # Fit the 2PL IRT model
  fit2PL <- mirt(data = df_FL, model = unimodel, itemtype = "2PL", verbose = FALSE)
  
  # Extract the factor scores (theta) as the index
  IV <- fscores(fit2PL)
  
  return(IV)
}

# Generate the IV index
df_Answers <- create_df_Answers(ecf_2016)
IV <- min_max_scale(create_index(df_Answers))
IV <- as.data.frame(IV)
IV <- IV$F1

a <- orderNorm(IV)
IV <- a$x.t
IV <- min_max_scale(IV)
rm(a)


# Clean up variables
rm(df_Answers, create_df_Answers, create_index)

################################################################################
### DEPENDENT VARIABLE (Y)
################################################################################

# Create the dependent variable (DV2) based on the values in column b0600, performing necessary transformations
DV2 <- ecf_2016 %>% select(b0600)
DV2 <- ifelse(DV2$b0600 == -98, NaN, DV2$b0600)  # Replace invalid values with NaN
DV2 <- ifelse(DV2 == 1 | DV2 == 2, 1, 0)  # Convert to binary values
DV2 <- na.omit(DV2)  # Remove missing values

################################################################################
### CONTROL VARIABLES (PCA)
################################################################################

# Perform PCA on selected sociodemographic variables and generate 4 components (I1, I2, I3, I4) as control variables.

# Create the dataset for PCA analysis
BE <- data.frame(
  Filter = ecf_2016$b0600,
  Q1 = ifelse(ecf_2016$d0101 > 0, ecf_2016$d0101, 0),
  Q2 = ifelse(ecf_2016$d0102 > 0, ecf_2016$d0102, 0),
  Q3 = ifelse(ecf_2016$d0103 > 0, ecf_2016$d0103, 0),
  Q4 = ifelse(ecf_2016$d0104 > 0, ecf_2016$d0104, 0),
  Q5 = ifelse(ecf_2016$d0105 > 0, ecf_2016$d0105, 0),
  Q6 = ifelse(ecf_2016$d0106 > 0, ecf_2016$d0106, 0),
  Q7 = ifelse(ecf_2016$d0107 > 0, ecf_2016$d0107, 0),
  Q8 = ifelse(ecf_2016$d0108 > 0, ecf_2016$d0108, 0),
  Q9 = ifelse(ecf_2016$d0109 > 0, ecf_2016$d0109, 0),
  Q10 = ifelse(ecf_2016$d0110 > 0, ecf_2016$d0110, 0),
  Q11 = ifelse(ecf_2016$d0111 > 0, ecf_2016$d0111, 0),
  Q12 = ifelse(ecf_2016$d0112 > 0, ecf_2016$d0112, 0)
)

# Remove invalid data
BE <- BE[BE$Filter != -98, ]

# Perform PCA analysis and extract 4 components
mat_cor <- hetcor(BE)$correlations  # Correlation matrix
fa_FL <- fa(BE, nfactors = 4, rotate = 'none', fm = 'pa', max.iter = 1, scores = "Bartlett")
df_Inst <- as.data.frame(fa_FL$scores)

# Apply Min-Max scaling to the factor scores
I1 <- min_max_scale(df_Inst$PA1)
I2 <- min_max_scale(df_Inst$PA2)
I3 <- min_max_scale(df_Inst$PA3)
I4 <- min_max_scale(df_Inst$PA4)

# Clean memory
rm(fa_FL, mat_cor, df_Inst, BE)

################################################################################
### INSTRUMENTAL VARIABLES
################################################################################

# Create the instrumental variables to address potential endogeneity in regression models.

# IV_1: Binary indicator based on the number of books in childhood (a1400 >= 4)
InsV_1 <- ifelse(ecf_2016$a1400 >= 4, 1, 0)

# IV_2: Binary indicator for specific university qualifications requiring mathematical or financial knowledge
InsV_2 <- ifelse(ecf_2016$a1200 %in% c(1, 2, 4, 5, 6), 1, 0)

df <- data.frame(  
  InsV_1 = InsV_1,
  InsV_2 = InsV_2,
  Filter = ecf_2016$b0600
)

df <- df[df$Filter != -98, ]

InsV_1 <- df$InsV_1
InsV_2 <- df$InsV_2

# Clean memory
rm(df)

# Clean memory after IV creation
rm(df)

################################################################################
### REGRESSION (IV Models)
################################################################################

# Perform Instrumental Variable (IV) regression to estimate the effect of the endogenous independent variable on the dependent variable.

# Model 2.1: Using Instrumental Variable 1 (Cultural Level)
data <- data.frame(
  Y = DV2,
  X = IV,
  Z = InsV_1,
  C_1 = I1,
  C_2 = I2,
  C_3 = I3,
  C_4 = I4
)

fitY.LX <- glm(Y ~ X + C_1 + C_2 + C_3 + C_4, family = "binomial", data = data)
fitX.LZ <- glm(X ~ Z + C_1 + C_2 + C_3 + C_4, data = data)
fitIV_ts <- ivglm(estmethod = "ts", fitX.LZ = fitX.LZ, fitY.LX = fitY.LX, data = data)
summary(fitIV_ts)

# Call:  
#  ivglm(estmethod = "ts", fitX.LZ = fitX.LZ, fitY.LX = fitY.LX, data = data)
#
# Coefficients: 
#  Estimate Std. Error z value Pr(>|z|)    
#  (Intercept)  -1.3600     0.5820  -2.337 0.019442 *  
#  X             1.5904     0.9363   1.699 0.089390 .  
#  C_1           1.2269     0.3055   4.016 5.92e-05 ***
#  C_2           0.1273     0.3255   0.391 0.695712    
#  C_3           1.0373     0.2690   3.856 0.000115 ***
#  C_4          -0.4121     0.3530  -1.167 0.243030    
#  ---
#  Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

coeficients <- as.data.table(fitIV_ts$est)
coeficients <- transpose(coeficients)

# Define the coefficients from the model
intercept <- coeficients$V1
beta_X <- coeficients$V2
beta_C1 <- coeficients$V3
beta_C2 <- coeficients$V4
beta_C3 <- coeficients$V5
beta_C4 <- coeficients$V6

# Calculate the linear predictor (logit) for each row
data$linear_predictor <- intercept + 
  (beta_X * data$X) + 
  (beta_C1 * data$C_1) + 
  (beta_C2 * data$C_2) + 
  (beta_C3 * data$C_3) + 
  (beta_C4 * data$C_4)

# Convert the linear predictor to predicted probabilities using the logistic function
data$predicted_prob <- 1 / (1 + exp(-data$linear_predictor))

# Run the Hosmer-Lemeshow test
hoslem_test <- hoslem.test(data$Y, data$predicted_prob)  # g=10 divides data into deciles

# Print the test result
print(hoslem_test)

# Model 2.2: Using Instrumental Variable 2 (Mathematical Baggage)
data <- data.frame(
  Y = DV2,                 # Dependent variable
  X = IV,                  # Endogenous independent variable
  Z = InsV_2,              # Instrumental variable
  C_1 = I1,                # Control variable 1
  C_2 = I2,                # Control variable 2
  C_3 = I3,                # Control variable 3
  C_4 = I4                 # Control variable 4  
)

fitY.LX <- glm(formula = Y ~ X + C_1 + C_2 + C_3 + C_4, family = "binomial", data = data)
fitX.LZ <- glm(formula = X ~ Z + C_1 + C_2 + C_3 + C_4, data = data)
fitIV_ts <- ivglm(estmethod="ts", fitX.LZ=fitX.LZ, fitY.LX=fitY.LX, data = data)
summary(fitIV_ts)

# Call:  
#  ivglm(estmethod = "ts", fitX.LZ = fitX.LZ, fitY.LX = fitY.LX, data = data)
#
# Coefficients: 
#  Estimate Std. Error z value Pr(>|z|)    
#  (Intercept)  -1.3709     0.4811  -2.849 0.004382 ** 
#  X             1.6113     0.6664   2.418 0.015615 *  
#  C_1           1.2252     0.2789   4.392 1.12e-05 ***
#  C_2           0.1256     0.2947   0.426 0.670065    
#  C_3           1.0355     0.2665   3.886 0.000102 ***
#  C_4          -0.4104     0.3487  -1.177 0.239265    
#  ---
#  Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

coeficients <- as.data.table(fitIV_ts$est)
coeficients <- transpose(coeficients)

# Define the coefficients from the model
intercept <- coeficients$V1
beta_X <- coeficients$V2
beta_C1 <- coeficients$V3
beta_C2 <- coeficients$V4
beta_C3 <- coeficients$V5
beta_C4 <- coeficients$V6

# Calculate the linear predictor (logit) for each row
data$linear_predictor <- intercept + 
  (beta_X * data$X) + 
  (beta_C1 * data$C_1) + 
  (beta_C2 * data$C_2) + 
  (beta_C3 * data$C_3) + 
  (beta_C4 * data$C_4)

# Convert the linear predictor to predicted probabilities using the logistic function
data$predicted_prob <- 1 / (1 + exp(-data$linear_predictor))

# Run the Hosmer-Lemeshow test
hoslem_test <- hoslem.test(data$Y, data$predicted_prob)  # g=10 divides data into deciles

# Print the test result
print(hoslem_test)

summary(DV2)
1-0.641
